{"id": "2509.19314", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19314", "abs": "https://arxiv.org/abs/2509.19314", "authors": ["Sirui Wu", "Daijin Yang"], "title": "Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias", "comment": "Accepted for publication in NCME-AIME 2025", "summary": "This study evaluates item neutralization assisted by the large language model\n(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was\nused to rewrite the International Personality Item Pool Big Five Measure\n(IPIP-BFM-50), and 203 participants completed either the original or\nneutralized form along with the Marlowe-Crowne Social Desirability Scale. The\nresults showed preserved reliability and a five-factor structure, with gains in\nConscientiousness and declines in Agreeableness and Openness. The correlations\nwith social desirability decreased for several items, but inconsistently.\nConfigural invariance held, though metric and scalar invariance failed.\nFindings support AI neutralization as a potential but imperfect bias-reduction\nmethod."}
{"id": "2509.19319", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19319", "abs": "https://arxiv.org/abs/2509.19319", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "comment": "Under review", "summary": "The recent shift toward the Health Level Seven Fast Healthcare\nInteroperability Resources (HL7 FHIR) standard opens a new frontier for\nclinical AI, demanding LLM agents to navigate complex, resource-based data\nmodels instead of conventional structured health data. However, existing\nbenchmarks have lagged behind this transition, lacking the realism needed to\nevaluate recent LLMs on interoperable clinical data. To bridge this gap, we\nintroduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical\nquestions in the HL7 FHIR standard. Using this benchmark, we systematically\nevaluate agentic frameworks, comparing different data retrieval strategies\n(direct FHIR API calls vs. specialized tools), interaction patterns\n(single-turn vs. multi-turn), and reasoning strategies (natural language vs.\ncode generation). Our experiments highlight the practical challenges of\nretrieving data from intricate FHIR resources and the difficulty of reasoning\nover them, both of which critically affect question answering performance. We\npublicly release the FHIR-AgentBench dataset and evaluation suite\n(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research\nand the development of robust, reliable LLM agents for clinical applications."}
{"id": "2509.19322", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19322", "abs": "https://arxiv.org/abs/2509.19322", "authors": ["Millie Vyas", "Timothy Blattner", "Alden Dima"], "title": "Readme_AI: Dynamic Context Construction for Large Language Models", "comment": null, "summary": "Despite being trained on significant amounts of data, Large Language Models\n(LLMs) can provide inaccurate or unreliable information in the context of a\nuser's specific query. Given query-specific context significantly improves the\nusefulness of its responses. In this paper, we present a specification that can\nbe used to dynamically build context for data sources. The data source owner\ncreates the file containing metadata for LLMs to use when reasoning about\ndataset-related queries. To demonstrate our proposed specification, we created\na prototype Readme_AI Model Context Protocol (MCP) server that retrieves the\nmetadata from the data source and uses it to dynamically build context. Some\nfeatures that make this specification dynamic are the extensible types that\nrepresent crawling web-pages, fetching data from data repositories, downloading\nand parsing publications, and general text. The context is formatted and\ngrouped using user-specified tags that provide clear contextual information for\nthe LLM to reason about the content. We demonstrate the capabilities of this\nearly prototype by asking the LLM about the NIST-developed Hedgehog library,\nfor which common LLMs often provides inaccurate and irrelevant responses\ncontaining hallucinations. With Readme_AI, the LLM receives enough context that\nit is now able to reason about the library and its use, and even generate code\ninterpolated from examples that were included in the Readme_AI file provided by\nHedgehog's developer. Our primary contribution is a extensible protocol for\ndynamically grounding LLMs in specialized, owner-provided data, enhancing\nresponses from LLMs and reducing hallucinations. The source code for the\nReadme_AI tool is posted here: https://github.com/usnistgov/readme_ai ."}
{"id": "2509.19323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19323", "abs": "https://arxiv.org/abs/2509.19323", "authors": ["V. S. Raghu Parupudi"], "title": "Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding", "comment": "submitted to AAAI 2026", "summary": "Vector comparison in high dimensions is a fundamental task in NLP, yet it is\ndominated by two baselines: the raw dot product, which is unbounded and\nsensitive to vector norms, and the cosine similarity, which discards magnitude\ninformation entirely. This paper challenges both standards by proposing and\nrigorously evaluating a new class of parameter-free, magnitude-aware similarity\nmetrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic\nTangent Similarity (HTS), designed to integrate vector magnitude and alignment\nin a more principled manner. To ensure that my findings are robust and\ngeneralizable, I conducted a comprehensive evaluation using four\nstate-of-the-art sentence embedding models (all-MiniLM-L6-v2,\nall-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across\na diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,\nand PAWS. Using the Wilcoxon signed-rank test for statistical significance, my\nresults are definitive: on the tasks requiring holistic semantic understanding\n(paraphrase and inference), both OS and HTS provide a statistically significant\nimprovement in Mean Squared Error over both the raw dot product and cosine\nsimilarity, regardless of the underlying embedding model.Crucially, my findings\ndelineate the specific domain of advantage for these metrics: for tasks\nrequiring holistic semantic understanding like paraphrase and inference, my\nmagnitude-aware metrics offer a statistically superior alternative. This\nsignificant improvement was not observed on benchmarks designed to test highly\nnuanced compositional semantics (SICK, STS-B), identifying the challenge of\nrepresenting compositional text as a distinct and important direction for\nfuture work."}
{"id": "2509.19325", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19325", "abs": "https://arxiv.org/abs/2509.19325", "authors": ["Jian Ouyang", "Arman T", "Ge Jin"], "title": "How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs", "comment": null, "summary": "This paper investigates the impact of incorrect data on the performance and\nsafety of large language models (LLMs), specifically gpt-4o, during supervised\nfine-tuning (SFT). Although LLMs become increasingly vital across broad domains\nlike finance, coding, law, and health, fine-tuning on incorrect data can lead\nto \"emergent misalignment,\" producing harmful or deceptive outputs unrelated to\nthe intended task. We evaluate gpt-4o models fine-tuned with varying ratios\n(10\\% to 90\\% correct) of both obviously and subtly incorrect data across four\ndomains: coding, finance, health, and legal. Our findings show that even modest\namounts of incorrect data (10-25\\%) dramatically degrade domain performance and\nnot moral alignment. A clear threshold of at least 50\\% correct data is needed\nfor models to consistently recover strong performance, though they rarely match\nthe robustness and safety of the base model, which exhibits near-perfect\nalignment and zero dangerous completions out-of-the-box. This research\nemphasizes that the cost of incorrect data is heavy, highlighting the critical\nneed for extremely high-quality data curation or, alternatively, leveraging\nrobust base models without unnecessary fine-tuning for high-stakes\napplications."}
{"id": "2509.19326", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19326", "abs": "https://arxiv.org/abs/2509.19326", "authors": ["Ruochi Li", "Haoxuan Zhang", "Edward Gehringer", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers", "comment": "Accepted as short paper at 25th IEEE International Conference on Data\n  Mining", "summary": "The surge in scientific submissions has placed increasing strain on the\ntraditional peer-review process, prompting the exploration of large language\nmodels (LLMs) for automated review generation. While LLMs demonstrate\ncompetence in producing structured and coherent feedback, their capacity for\ncritical reasoning, contextual grounding, and quality sensitivity remains\nlimited. To systematically evaluate these aspects, we propose a comprehensive\nevaluation framework that integrates semantic similarity analysis and\nstructured knowledge graph metrics to assess LLM-generated reviews against\nhuman-written counterparts. We construct a large-scale benchmark of 1,683\npapers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and\ngenerate reviews using five LLMs. Our findings show that LLMs perform well in\ndescriptive and affirmational content, capturing the main contributions and\nmethodologies of the original work, with GPT-4o highlighted as an illustrative\nexample, generating 15.74% more entities than human reviewers in the strengths\nsection of good papers in ICLR 2025. However, they consistently underperform in\nidentifying weaknesses, raising substantive questions, and adjusting feedback\nbased on paper quality. GPT-4o produces 59.42% fewer entities than real\nreviewers in the weaknesses and increases node count by only 5.7% from good to\nweak papers, compared to 50% in human reviews. Similar trends are observed\nacross all conferences, years, and models, providing empirical foundations for\nunderstanding the merits and defects of LLM-generated reviews and informing the\ndevelopment of future LLM-assisted reviewing tools. Data, code, and more\ndetailed results are publicly available at\nhttps://github.com/RichardLRC/Peer-Review."}
{"id": "2509.19327", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19327", "abs": "https://arxiv.org/abs/2509.19327", "authors": ["Braxton A. Morrison", "Madhumita Sushil", "Jacob S. Young"], "title": "A systematic review of trial-matching pipelines using large language models", "comment": "28 pages, 3 figures", "summary": "Matching patients to clinical trial options is critical for identifying novel\ntreatments, especially in oncology. However, manual matching is labor-intensive\nand error-prone, leading to recruitment delays. Pipelines incorporating large\nlanguage models (LLMs) offer a promising solution. We conducted a systematic\nreview of studies published between 2020 and 2025 from three academic databases\nand one preprint server, identifying LLM-based approaches to clinical trial\nmatching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies\nfocused on matching patient-to-criterion only (n=4), patient-to-trial only\n(n=10), trial-to-patient only (n=2), binary eligibility classification only\n(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real\npatient data; one used both. Variability in datasets and evaluation metrics\nlimited cross-study comparability. In studies with direct comparisons, the\nGPT-4 model consistently outperformed other models, even finely-tuned ones, in\nmatching and eligibility extraction, albeit at higher cost. Promising\nstrategies included zero-shot prompting with proprietary LLMs like the GPT-4o\nmodel, advanced retrieval methods, and fine-tuning smaller, open-source models\nfor data privacy when incorporation of large models into hospital\ninfrastructure is infeasible. Key challenges include accessing sufficiently\nlarge real-world data sets, and deployment-associated challenges such as\nreducing cost, mitigating risk of hallucinations, data leakage, and bias. This\nreview synthesizes progress in applying LLMs to clinical trial matching,\nhighlighting promising directions and key limitations. Standardized metrics,\nmore realistic test sets, and attention to cost-efficiency and fairness will be\ncritical for broader deployment."}
{"id": "2509.19329", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.19329", "abs": "https://arxiv.org/abs/2509.19329", "authors": ["Julie Jung", "Max Lu", "Sina Chole Benker", "Dogus Darici"], "title": "How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment", "comment": "9 pages, 4 figures, accepted at NCME AIME 2025", "summary": "We examined how model size, temperature, and prompt style affect Large\nLanguage Models' (LLMs) alignment within itself, between models, and with human\nin assessing clinical reasoning skills. Model size emerged as a key factor in\nLLM-human score alignment. Study highlights the importance of checking\nalignments across multiple levels."}
{"id": "2509.19332", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19332", "abs": "https://arxiv.org/abs/2509.19332", "authors": ["Zhijin Guo", "Chenhao Xue", "Zhaozhen Xu", "Hongbo Bo", "Yuxuan Ye", "Janet B. Pierrehumbert", "Martha Lewis"], "title": "Quantifying Compositionality of Classic and State-of-the-Art Embeddings", "comment": "Findings of the Association for Computational Linguistics: EMNLP 2025", "summary": "For language models to generalize correctly to novel expressions, it is\ncritical that they exploit access compositional meanings when this is\njustified. Even if we don't know what a \"pelp\" is, we can use our knowledge of\nnumbers to understand that \"ten pelps\" makes more pelps than \"two pelps\".\nStatic word embeddings such as Word2vec made strong, indeed excessive, claims\nabout compositionality. The SOTA generative, transformer models and graph\nmodels, however, go too far in the other direction by providing no real limits\non shifts in meaning due to context. To quantify the additive compositionality,\nwe formalize a two-step, generalized evaluation that (i) measures the linearity\nbetween known entity attributes and their embeddings via canonical correlation\nanalysis, and (ii) evaluates additive generalization by reconstructing\nembeddings for unseen attribute combinations and checking reconstruction\nmetrics such as L2 loss, cosine similarity, and retrieval accuracy. These\nmetrics also capture failure cases where linear composition breaks down.\nSentences, knowledge graphs, and word embeddings are evaluated and tracked the\ncompositionality across all layers and training stages. Stronger compositional\nsignals are observed in later training stages across data modalities, and in\ndeeper layers of the transformer-based model before a decline at the top layer.\nCode is available at\nhttps://github.com/Zhijin-Guo1/quantifying-compositionality."}
{"id": "2509.19333", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19333", "abs": "https://arxiv.org/abs/2509.19333", "authors": ["Chengkai Huang", "Junda Wu", "Zhouhang Xie", "Yu Xia", "Rui Wang", "Tong Yu", "Subrata Mitra", "Julian McAuley", "Lina Yao"], "title": "Pluralistic Off-policy Evaluation and Alignment", "comment": null, "summary": "Personalized preference alignment for LLMs with diverse human preferences\nrequires evaluation and alignment methods that capture pluralism. Most existing\npreference alignment datasets are logged under policies that differ\nsubstantially from the evaluated LLMs, and existing off-policy estimators focus\nsolely on overall utility while ignoring preference pluralism. Extending\nOff-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,\nremains an open question. Thus, we propose the Pluralistic Off-Policy\nEvaluation (POPE), the first framework for offline pluralistic preference\nevaluation and alignment in LLMs. POPE includes a unified reward function that\ncombines (1) a collaborative utility component derived from human preference\nsignals (e.g., upvotes or relevance scores) and (2) a diversity component\ninspired by entropy-based coverage measures, together reflecting pluralistic\nalignment. Furthermore, to estimate this reward from logged interactions, we\nderive decomposable inverse propensity scoring (IPS) estimators that separately\nevaluate relevance and diversity. Theoretically, we prove that our decomposed\nIPS estimators establish a lower bound on their variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance pluralistic alignment. Empirical results demonstrate that POPE\nefficiently enhances pluralistic response generation and maintains the models'\ngeneral capabilities on downstream tasks"}
{"id": "2509.19336", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19336", "abs": "https://arxiv.org/abs/2509.19336", "authors": ["Qingsong Wang", "Tao Wu", "Wang Lin", "Yueying Feng", "Gongsheng Yuan", "Chang Yao", "Jingyuan Chen"], "title": "Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation", "comment": "Accepted to Findings of EMNLP 2026", "summary": "Large Language Models (LLMs) have demonstrated strong performance in\nopen-ended generation tasks. However, they often struggle to adapt content to\nusers with differing cognitive capacities, leading to a phenomenon we term\ncognitive misalignment. This issue arises in two forms: knowledge-level\nmisalignment, where content is too complex or too simplistic relative to user\nunderstanding, and presentation-style misalignment, where the structure or tone\nhinders effective comprehension. To address these challenges, we propose the\nCognitive-Level Alignment Framework (CLAF), a general-purpose generation\nframework that aligns both knowledge complexity and presentation style with\nuser cognition. CLAF integrates a capability-aware retrieval module based on a\nhierarchical knowledge graph and a style optimization module guided by Bloom's\ntaxonomy and preference learning. Additionally, a knowledge-controllable\ngeneration component ensures consistency and relevance throughout the output.\nTo support training and evaluation, we construct SCALE, a cognitively annotated\ndataset containing responses at multiple comprehension levels per query.\nEmpirical results show that CLAF enhances the adaptability and informativeness\nof LLM outputs across a range of user profiles, offering a robust solution to\ncognitive-level alignment in real-world applications."}
{"id": "2509.19343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19343", "abs": "https://arxiv.org/abs/2509.19343", "authors": ["Alovi N Shohe", "Chonglio Khiamungam", "Teisovi Angami"], "title": "Part-of-speech tagging for Nagamese Language using CRF", "comment": "8 pages", "summary": "This paper investigates part-of-speech tagging, an important task in Natural\nLanguage Processing (NLP) for the Nagamese language. The Nagamese language,\na.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily\nas a means of communication in trade between the Nagas and people from Assam in\nnortheast India. A substantial amount of work in part-of-speech-tagging has\nbeen done for resource-rich languages like English, Hindi, etc. However, no\nwork has been done in the Nagamese language. To the best of our knowledge, this\nis the first attempt at part-of-speech tagging for the Nagamese Language. The\naim of this work is to identify the part-of-speech for a given sentence in the\nNagamese language. An annotated corpus of 16,112 tokens is created and applied\nmachine learning technique known as Conditional Random Fields (CRF). Using CRF,\nan overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score\nof 85% is achieved.\n  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF."}
{"id": "2509.19344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19344", "abs": "https://arxiv.org/abs/2509.19344", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "An-Kwok Ian Wong", "Neal Chaisson", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Performance of Large Language Models in Answering Critical Care Medicine Questions", "comment": null, "summary": "Large Language Models have been tested on medical student-level questions,\nbut their performance in specialized fields like Critical Care Medicine (CCM)\nis less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B\nparameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%\naverage accuracy. Performance varied across domains, highest in Research\n(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future\nwork to improve models across various subspecialty domains."}
{"id": "2509.19345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19345", "abs": "https://arxiv.org/abs/2509.19345", "authors": ["Renyu Li", "Antonio Jimeno Yepes", "Yao You", "Kamil Pluci≈Ñski", "Maximilian Operlejn", "Crag Wolfe"], "title": "SCORE: A Semantic Evaluation Framework for Generative Document Parsing", "comment": null, "summary": "Multi-modal generative document parsing systems challenge traditional\nevaluation: unlike deterministic OCR or layout models, they often produce\nsemantically correct yet structurally divergent outputs. Conventional\nmetrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing\nvalid interpretations and obscuring system behavior.\n  We introduce SCORE (Structural and COntent Robust Evaluation), an\ninterpretation-agnostic framework that integrates (i) adjusted edit distance\nfor robust content fidelity, (ii) token-level diagnostics to distinguish\nhallucinations from omissions, (iii) table evaluation with spatial tolerance\nand semantic alignment, and (iv) hierarchy-aware consistency checks. Together,\nthese dimensions enable evaluation that embraces representational diversity\nwhile enforcing semantic rigor.\n  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE\nconsistently revealed cross-dataset performance patterns missed by standard\nmetrics. In 2-5% of pages with ambiguous table structures, traditional metrics\npenalized systems by 12-25% on average, leading to distorted rankings. SCORE\ncorrected these cases, recovering equivalence between alternative but valid\ninterpretations. Moreover, by normalizing generative outputs into a\nformat-agnostic representation, SCORE reproduces traditional scores (e.g.,\ntable F1 up to 0.93) without requiring object-detection pipelines,\ndemonstrating that generative parsing alone suffices for comprehensive\nevaluation.\n  By exposing how interpretive diversity impacts evaluation outcomes and\nproviding multi-dimensional, interpretable diagnostics, SCORE establishes\nfoundational principles for semantically grounded, fair, and practical\nbenchmarking of modern document parsing systems."}
{"id": "2509.19346", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19346", "abs": "https://arxiv.org/abs/2509.19346", "authors": ["Maryam Mahdi Alhusseini", "Mohammad-Reza Feizi-Derakhshi"], "title": "Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches", "comment": "17 pages, 21 figures", "summary": "This study presents a novel dual-perspective approach to analyzing user\nreviews for ChatGPT and DeepSeek on the Google Play Store, integrating\nlexicon-based sentiment analysis (TextBlob) with deep learning classification\nmodels, including Convolutional Neural Networks (CNN) and Bidirectional Long\nShort Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on\neither lexicon-based strategies or predictive deep learning models in\nisolation, this study conducts an extensive investigation into user\nsatisfaction with Large Language Model (LLM) based applications. A Dataset of\n4,000 authentic user reviews was collected, which were carefully preprocessed\nand subjected to oversampling to achieve balanced classes. The balanced test\nset of 1,700 Reviews were used for model testing. Results from the experiments\nreveal that ChatGPT received significantly more positive sentiment than\nDeepSeek. Furthermore, deep learning based classification demonstrated superior\nperformance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving\n96.41 percent accuracy and near perfect classification of negative reviews,\nalongside high F1-scores for neutral and positive sentiments. This research\nsets a new methodological standard for measuring sentiment in LLM-based\napplications and provides practical insights for developers and researchers\nseeking to improve user-centric AI system design."}
{"id": "2509.19347", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19347", "abs": "https://arxiv.org/abs/2509.19347", "authors": ["Sara Todorovikj", "Lars-Peter Meyer", "Michael Martin"], "title": "Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks", "comment": "peer reviewed publication at SEMANTiCS 2025 Poster Track", "summary": "Large Language Models (LLMs) are increasingly used for tasks involving\nKnowledge Graphs (KGs), whose evaluation typically focuses on accuracy and\noutput correctness. We propose a complementary task characterization approach\nusing three complexity frameworks from cognitive psychology. Applying this to\nthe LLM-KG-Bench framework, we highlight value distributions, identify\nunderrepresented demands and motivate richer interpretation and diversity for\nbenchmark evaluation tasks."}
{"id": "2509.19349", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19349", "abs": "https://arxiv.org/abs/2509.19349", "authors": ["Robert Tjarko Lange", "Yuki Imajuku", "Edoardo Cetin"], "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution", "comment": "52 pages, 14 figures", "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large\nlanguage models (LLMs) to advance scientific discovery with state-of-the-art\nperformance and unprecedented efficiency. Recent advances in scaling inference\ntime compute of LLMs have enabled significant progress in generalized\nscientific discovery. These approaches rely on evolutionary agentic harnesses\nthat leverage LLMs as mutation operators to generate candidate solutions.\nHowever, current code evolution methods suffer from critical limitations: they\nare sample inefficient, requiring thousands of samples to identify effective\nsolutions, and remain closed-source, hindering broad adoption and extension.\nShinkaEvolve addresses these limitations, introducing three key innovations: a\nparent sampling technique balancing exploration and exploitation, code novelty\nrejection-sampling for efficient search space exploration, and a bandit-based\nLLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,\ndemonstrating consistent improvements in sample efficiency and solution\nquality. ShinkaEvolve discovers a new state-of-the-art circle packing solution\nusing only 150 samples, designs high-performing agentic harnesses for AIME\nmathematical reasoning tasks, identifies improvements to ALE-Bench competitive\nprogramming solutions, and discovers novel mixture-of-expert load balancing\nloss functions that illuminate the space of optimization strategies. Our\nresults demonstrate that ShinkaEvolve achieves broad applicability with\nexceptional sample efficiency. By providing open-source accessibility and\ncost-efficiency, this work democratizes open-ended discovery across diverse\ncomputational problems."}
{"id": "2509.19352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19352", "abs": "https://arxiv.org/abs/2509.19352", "authors": ["Jiajun Chen", "Yangyang Wu", "Xiaoye Miao", "Mengying Zhu", "Meng Xi"], "title": "TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities", "comment": null, "summary": "The widespread presence of incomplete modalities in multimodal data poses a\nsignificant challenge to achieving accurate rumor detection. Existing\nmultimodal rumor detection methods primarily focus on learning joint modality\nrepresentations from \\emph{complete} multimodal training data, rendering them\nineffective in addressing the common occurrence of \\emph{missing modalities} in\nreal-world scenarios. In this paper, we propose a hierarchical soft prompt\nmodel \\textsf{TriSPrompt}, which integrates three types of prompts,\n\\textit{i.e.}, \\emph{modality-aware} (MA) prompt, \\emph{modality-missing} (MM)\nprompt, and \\emph{mutual-views} (MV) prompt, to effectively detect rumors in\nincomplete multimodal data. The MA prompt captures both heterogeneous\ninformation from specific modalities and homogeneous features from available\ndata, aiding in modality recovery. The MM prompt models missing states in\nincomplete data, enhancing the model's adaptability to missing information. The\nMV prompt learns relationships between subjective (\\textit{i.e.}, text and\nimage) and objective (\\textit{i.e.}, comments) perspectives, effectively\ndetecting rumors. Extensive experiments on three real-world benchmarks\ndemonstrate that \\textsf{TriSPrompt} achieves an accuracy gain of over 13\\%\ncompared to state-of-the-art methods. The codes and datasets are available at\nhttps: //anonymous.4open.science/r/code-3E88."}
{"id": "2509.19354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19354", "abs": "https://arxiv.org/abs/2509.19354", "authors": ["Ahmed El Fekih Zguir", "Ferda Ofli", "Muhammad Imran"], "title": "RoadMind: Towards a Geospatial AI Expert for Disaster Response", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across a range\nof natural language tasks, but remain limited in their ability to reason about\ngeospatial data, particularly road networks, distances, and directions. This\ngap poses challenges in disaster scenarios, where spatial understanding is\ncritical for tasks such as evacuation planning and resource allocation. In this\nwork, we present RoadMind, a self-supervised framework that enhances the\ngeospatial reasoning capabilities of LLMs using structured data from\nOpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data\nfor a given city and converts it into multiple supervision formats tailored to\nkey spatial tasks. We pretrain and fine-tune LLMs on these representations\nusing QLoRA adapters and 4-bit quantized models. We evaluate our approach on\nthree disaster-prone cities with varying global representation, Los Angeles,\nChristchurch, and Manila, across tasks such as road segment identification,\nnearest road retrieval, and distance/direction estimation. Our results show\nthat models trained via RoadMind significantly outperform strong baselines,\nincluding state-of-the-art LLMs equipped with advanced prompt engineering. This\ndemonstrates the potential of structured geospatial data to enhance language\nmodels with robust spatial reasoning, enabling more effective offline AI\nsystems for disaster response."}
{"id": "2509.19358", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19358", "abs": "https://arxiv.org/abs/2509.19358", "authors": ["Chimaobi Okite", "Naihao Deng", "Kiran Bodipati", "Huaidian Hou", "Joyce Chai", "Rada Mihalcea"], "title": "Benchmarking and Improving LLM Robustness for Personalized Generation", "comment": "First draft. First camera-ready version", "summary": "Recent years have witnessed a growing interest in personalizing the responses\nof large language models (LLMs). While existing evaluations primarily focus on\nwhether a response aligns with a user's preferences, we argue that factuality\nis an equally important yet often overlooked dimension. In the context of\npersonalization, we define a model as robust if its responses are both\nfactually accurate and align with the user preferences. To assess this, we\nintroduce PERG, a scalable framework for evaluating robustness in LLMs, along\nwith a new dataset, PERGData. We evaluate fourteen models from five different\nmodel families using different prompting methods. Our findings show that\ncurrent LLMs struggle with robust personalization: even the strongest models\n(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously\nsuccessful cases without personalization, while smaller models (e.g., 7B-scale)\ncan fail more than 20% of the time. Further analysis reveals that robustness is\nsignificantly affected by the nature of the query and the type of user\npreference. To mitigate these failures, we propose Pref-Aligner, a two-stage\napproach that improves robustness by an average of 25% across models. Our work\nhighlights critical gaps in current evaluation practices and introduces tools\nand metrics to support more reliable, user-aligned LLM deployments."}
{"id": "2509.19360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19360", "abs": "https://arxiv.org/abs/2509.19360", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Semantic Representation Attack against Aligned Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available."}
{"id": "2509.19364", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19364", "abs": "https://arxiv.org/abs/2509.19364", "authors": ["Angelina Wang", "Daniel E. Ho", "Sanmi Koyejo"], "title": "The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior", "comment": "forthcoming in Patterns", "summary": "Standard offline evaluations for language models -- a series of independent,\nstate-less inferences made by models -- fail to capture how language models\nactually behave in practice, where personalization fundamentally alters model\nbehavior. For instance, identical benchmark questions to the same language\nmodel can produce markedly different responses when prompted to a state-less\nsystem, in one user's chat session, or in a different user's chat session. In\nthis work, we provide empirical evidence showcasing this phenomenon by\ncomparing offline evaluations to field evaluations conducted by having 800 real\nusers of ChatGPT and Gemini pose benchmark and other provided questions to\ntheir chat interfaces."}
{"id": "2509.19365", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19365", "abs": "https://arxiv.org/abs/2509.19365", "authors": ["Wannes Janssens", "Matthias Bogaert", "Dirk Van den Poel"], "title": "LLM-Assisted Topic Reduction for BERTopic on Social Media Data", "comment": "13 pages, 8 figures. To be published in the Post-Workshop proceedings\n  of the ECML PKDD 2025 Conference", "summary": "The BERTopic framework leverages transformer embeddings and hierarchical\nclustering to extract latent topics from unstructured text corpora. While\neffective, it often struggles with social media data, which tends to be noisy\nand sparse, resulting in an excessive number of overlapping topics. Recent work\nexplored the use of large language models for end-to-end topic modelling.\nHowever, these approaches typically require significant computational overhead,\nlimiting their scalability in big data contexts. In this work, we propose a\nframework that combines BERTopic for topic generation with large language\nmodels for topic reduction. The method first generates an initial set of topics\nand constructs a representation for each. These representations are then\nprovided as input to the language model, which iteratively identifies and\nmerges semantically similar topics. We evaluate the approach across three\nTwitter/X datasets and four different language models. Our method outperforms\nthe baseline approach in enhancing topic diversity and, in many cases,\ncoherence, with some sensitivity to dataset characteristics and initial\nparameter selection."}
{"id": "2509.19368", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19368", "abs": "https://arxiv.org/abs/2509.19368", "authors": ["Ruanjun Li", "Ziheng Liu", "Yuanming Shi", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding", "comment": "17 pages, 7 figures", "summary": "Large language models (LLMs) deliver impressive generation quality, but incur\nvery high inference cost because each output token is generated\nauto-regressively through all model layers. Early-exit based self-speculative\ndecoding (EESD) has emerged to mitigate this cost. However, in practice, many\napproaches struggle to achieve the expected acceleration in such\ndraft-then-verify paradigm even with a well-aligned early-exit head and\nselected exit position. Our analysis reveals that EESD only pays off when the\nvast majority of draft tokens are accepted by the LLM. Otherwise, the draft\ncost may overcome the acceleration gain and lead to a negative speedup. To\nmitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)\nthat fully pipelines the draft and verification work so that no effort is\nwasted on failed predictions. It has two key innovations. We configure the\nmodel layers as a pipeline in which early-exit (draft) computations and\nremaining-layer (verification) computations overlap. We interleave drafting and\nverification per token. While the LLM is verifying the current token in its\nfinal layers, the early-exit path simultaneously drafts the next token. Such a\nverify-while-draft scheme keeps all units busy and validates tokens on-the-fly\nanalogous to pipelining the speculation and verification stages. Empirical\nresults confirm that PPSD achieves state-of-the-art acceleration in\nself-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup\nratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration\nat the fixed acceptance rate and exit position, showcasing its advancement in\nproviding efficient self-speculation."}
{"id": "2509.19369", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19369", "abs": "https://arxiv.org/abs/2509.19369", "authors": ["Changhyun Jeon", "Jinhee Park", "Jungwoo Choi", "Keonwoo Kim", "Jisu Kim", "Minji Hong"], "title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use", "comment": null, "summary": "We propose a small-scale language model (SLM) based agent architecture,\nPlanner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G\nseparates planning, calling, and generation by role: the Planner produces an\ninitial batch plan with limited on-demand replanning; the Caller returns a\nnormalized call object after joint schema-value validation; and the Generator\nintegrates tool outputs to produce the final answer. We apply a Korean-first\nvalue policy to reduce execution failures caused by frequent Korean-to-English\ncode switching in Korean settings. Evaluation assumes Korean queries and Korean\ntool/parameter specifications; it covers single-chain, multi-chain,\nmissing-parameters, and missing-functions scenarios, and is conducted via an\nLLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.\nResults show that P-C-G delivers competitive tool-use accuracy and end-to-end\nquality while reducing tokens and maintaining acceptable latency, indicating\nthat role-specialized SLMs are a cost-effective alternative for Korean tool-use\nagents."}
{"id": "2509.19370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19370", "abs": "https://arxiv.org/abs/2509.19370", "authors": ["Zhaoyu Ma", "Yuan Shan", "Jiahao Zhao", "Nan Xu", "Lei Wang"], "title": "Meow: End-to-End Outline Writing for Automatic Academic Survey", "comment": null, "summary": "As academic paper publication numbers grow exponentially, conducting in-depth\nsurveys with LLMs automatically has become an inevitable trend. Outline\nwriting, which aims to systematically organize related works, is critical for\nautomated survey generation. Yet existing automatic survey methods treat\noutline writing as mere workflow steps in the overall pipeline. Such\ntemplate-based workflows produce outlines that lack in-depth understanding of\nthe survey topic and fine-grained styles. To address these limitations, we\npropose Meow, the first metadata-driven outline writing framework that produces\norganized and faithful outlines efficiently. Specifically, we first formulate\noutline writing as an end-to-end task that generates hierarchical structured\noutlines from paper metadata. We then curate a high-quality dataset of surveys\nfrom arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics\nfor outline quality assessment. Finally, we employ a two-stage training\napproach combining supervised fine-tuning and reinforcement learning. Our 8B\nreasoning model demonstrates strong performance with high structural fidelity\nand stylistic coherence."}
{"id": "2509.19371", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19371", "abs": "https://arxiv.org/abs/2509.19371", "authors": ["Kangtao Lv", "Haibin Chen", "Yujin Yuan", "Langming Liu", "Shilei Liu", "Yongwei Wang", "Wenbo Su", "Bo Zheng"], "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models", "comment": null, "summary": "Large language models (LLMs) have attracted significant attention due to\ntheir impressive general capabilities across diverse downstream tasks. However,\nwithout domain-specific optimization, they often underperform on specialized\nknowledge benchmarks and even produce hallucination. Recent studies show that\nstrategically infusing domain knowledge during pretraining can substantially\nimprove downstream performance. A critical challenge lies in balancing this\ninfusion trade-off: injecting too little domain-specific data yields\ninsufficient specialization, whereas excessive infusion triggers catastrophic\nforgetting of previously acquired knowledge. In this work, we focus on the\nphenomenon of memory collapse induced by over-infusion. Through systematic\nexperiments, we make two key observations, i.e. 1) Critical collapse point:\neach model exhibits a threshold beyond which its knowledge retention\ncapabilities sharply degrade. 2) Scale correlation: these collapse points scale\nconsistently with the model's size. Building on these insights, we propose a\nknowledge infusion scaling law that predicts the optimal amount of domain\nknowledge to inject into large LLMs by analyzing their smaller counterparts.\nExtensive experiments across different model sizes and pertaining token budgets\nvalidate both the effectiveness and generalizability of our scaling law."}
{"id": "2509.19476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19476", "abs": "https://arxiv.org/abs/2509.19476", "authors": ["Yutaro Sigris", "Andreas Waldis"], "title": "A Pipeline to Assess Merging Methods via Behavior and Internals", "comment": "BlackboxNLP", "summary": "Merging methods combine the weights of multiple language models (LMs) to\nleverage their capacities, such as for domain adaptation. While existing\nstudies investigate merged models from a solely behavioral perspective, we\noffer the first comprehensive view by assessing and connecting their behavior\nand internals. We present a novel evaluation pipeline that first merges\nmultiple parent LMs, and then evaluates the merged models in comparison to the\ninitial ones based on their behavior on downstream tasks, like MMLU, and the\ninternal encoded linguistic competence. We showcase this pipeline by assessing\nthe merging of instruction fine-tuned with math- and code-adapted LMs from the\nQwen2.5 family. Our results show that merging methods impacts behavior and\ninternals differently. While the performance of merged models is typically\nbetween that of the two parent models, their encoded information about\nlinguistic phenomena, particularly in morphology and syntax, can surpass the\nparent models. Moreover, we find weak ranking correlation between this behavior\nand internal evaluation. With our pipeline and initial results, we emphasize\nthe need for more comprehensive evaluations of model merging methods to gain a\nfaithful understanding of their capabilities and reliability, beyond potential\nsuperficial behavioral advances."}
{"id": "2509.19540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19540", "abs": "https://arxiv.org/abs/2509.19540", "authors": ["Jayanth Krishna Chundru", "Rudrashis Poddar", "Jie Cao", "Tianyu Jiang"], "title": "Do LLMs Encode Frame Semantics? Evidence from Frame Identification", "comment": null, "summary": "We investigate whether large language models encode latent knowledge of frame\nsemantics, focusing on frame identification, a core challenge in frame semantic\nparsing that involves selecting the appropriate semantic frame for a target\nword in context. Using the FrameNet lexical resource, we evaluate models under\nprompt-based inference and observe that they can perform frame identification\neffectively even without explicit supervision. To assess the impact of\ntask-specific training, we fine-tune the model on FrameNet data, which\nsubstantially improves in-domain accuracy while generalizing well to\nout-of-domain benchmarks. Further analysis shows that the models can generate\nsemantically coherent frame definitions, highlighting the model's internalized\nunderstanding of frame semantics."}
{"id": "2509.19557", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19557", "abs": "https://arxiv.org/abs/2509.19557", "authors": ["Iris Kamsteeg", "Juan Cardenas-Cartagena", "Floris van Beers", "Gineke ten Holt", "Tsegaye Misikir Tashu", "Matias Valdenegro-Toro"], "title": "Confidence Calibration in Large Language Model-Based Entity Matching", "comment": "9 pages, 2 figures. UncertaiNLP 2025 Workshop @ EMNLP Camera Ready", "summary": "This research aims to explore the intersection of Large Language Models and\nconfidence calibration in Entity Matching. To this end, we perform an empirical\nstudy to compare baseline RoBERTa confidences for an Entity Matching task\nagainst confidences that are calibrated using Temperature Scaling, Monte Carlo\nDropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company\ndatasets. The findings indicate that the proposed modified RoBERTa model\nexhibits a slight overconfidence, with Expected Calibration Error scores\nranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence\ncan be mitigated using Temperature Scaling, reducing Expected Calibration Error\nscores by up to 23.83%."}
{"id": "2509.19563", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19563", "abs": "https://arxiv.org/abs/2509.19563", "authors": ["Stefania Radu", "Marco Zullich", "Matias Valdenegro-Toro"], "title": "Uncertainty in Semantic Language Modeling with PIXELS", "comment": "9 pages, 6 figures, UncertaiNLP 2025 Workshop @ EMNLP Camera Ready", "summary": "Pixel-based language models aim to solve the vocabulary bottleneck problem in\nlanguage modeling, but the challenge of uncertainty quantification remains\nopen. The novelty of this work consists of analysing uncertainty and confidence\nin pixel-based language models across 18 languages and 7 scripts, all part of 3\nsemantically challenging tasks. This is achieved through several methods such\nas Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The\nresults suggest that pixel-based models underestimate uncertainty when\nreconstructing patches. The uncertainty is also influenced by the script, with\nLatin languages displaying lower uncertainty. The findings on ensemble learning\nshow better performance when applying hyperparameter tuning during the named\nentity recognition and question-answering tasks across 16 languages."}
{"id": "2509.19567", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.19567", "abs": "https://arxiv.org/abs/2509.19567", "authors": ["Dimitrios Siskos", "Stavros Papadopoulos", "Pablo Peso Parada", "Jisi Zhang", "Karthikeyan Saravanan", "Anastasios Drosou"], "title": "Retrieval Augmented Generation based context discovery for ASR", "comment": "Accepted at EMNLP 2025", "summary": "This work investigates retrieval augmented generation as an efficient\nstrategy for automatic context discovery in context-aware Automatic Speech\nRecognition (ASR) system, in order to improve transcription accuracy in the\npresence of rare or out-of-vocabulary terms. However, identifying the right\ncontext automatically remains an open challenge. This work proposes an\nefficient embedding-based retrieval approach for automatic context discovery in\nASR. To contextualize its effectiveness, two alternatives based on large\nlanguage models (LLMs) are also evaluated: (1) large language model (LLM)-based\ncontext generation via prompting, and (2) post-recognition transcript\ncorrection using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech\ndemonstrate that the proposed approach reduces WER by up to 17% (percentage\ndifference) relative to using no-context, while the oracle context results in a\nreduction of up to 24.1%."}
{"id": "2509.19569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19569", "abs": "https://arxiv.org/abs/2509.19569", "authors": ["Aleksis Datseris", "Sylvia Vassileva", "Ivan Koychev", "Svetla Boytcheva"], "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities", "comment": null, "summary": "This paper introduces a novel approach to position embeddings in transformer\nmodels, named \"Exact Positional Embeddings\" (ExPE). An absolute positional\nembedding method that can extrapolate to sequences of lengths longer than the\nones it was trained on. Traditional transformer models rely on absolute or\nrelative position embeddings to incorporate positional information into token\nembeddings, which often struggle with extrapolation to sequences longer than\nthose seen during training. Our proposed method utilizes a novel embedding\nstrategy that encodes exact positional information by overriding specific\ndimensions of the embedding vectors, thereby enabling a more precise\nrepresentation of token positions. The proposed approach not only maintains the\nintegrity of the original embeddings but also enhances the model's ability to\ngeneralize to more extended sequences. In causal language modeling, our ExPE\nembeddings significantly reduce perplexity compared to rotary and sinusoidal\nembeddings, when tested on sequences longer than those used in training."}
{"id": "2509.19580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19580", "abs": "https://arxiv.org/abs/2509.19580", "authors": ["Yanfang", "Ye", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Yiyang Li", "Shifu Hou", "Weixiang Sun", "Kaiwen Shi", "Yijun Ma", "Wei Song", "Ahmed Abbasi", "Ying Cheng", "Jane Cleland-Huang", "Steven Corcelli", "Patricia Culligan", "Robert Goulding", "Ming Hu", "Ting Hua", "John Lalor", "Fang Liu", "Tengfei Luo", "Ed Maginn", "Nuno Moniz", "Jason Rohr", "Brett Savoie", "Daniel Slate", "Tom Stapleford", "Matthew Webber", "Olaf Wiest", "Johnny Zhang", "Nitesh Chawla"], "title": "LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines", "comment": null, "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications."}
{"id": "2509.19593", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19593", "abs": "https://arxiv.org/abs/2509.19593", "authors": ["Dylan Hutson", "Daniel Vennemeyer", "Aneesh Deshmukh", "Justin Zhan", "Tianyu Jiang"], "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models", "comment": "EMNLP 2025, 17 pages, 2 figures", "summary": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning."}
{"id": "2509.19595", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19595", "abs": "https://arxiv.org/abs/2509.19595", "authors": ["Mohammad Saim", "Phan Anh Duong", "Cat Luong", "Aniket Bhanderi", "Tianyu Jiang"], "title": "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models", "comment": null, "summary": "The embodiment of emotional reactions from body parts contains rich\ninformation about our affective experiences. We propose a framework that\nutilizes state-of-the-art large vision-language models (LVLMs) to generate\nEmbodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered\ntext outputs, primarily comprising descriptions that focus on the salient body\nparts involved in emotional reactions. We also employ attention maps and\nobserve that contemporary models exhibit a persistent bias towards the facial\nregion. Despite this limitation, we observe that our employed framework can\neffectively recognize embodied emotions in face-masked images, outperforming\nbaselines without any fine-tuning. ELENA opens a new trajectory for embodied\nemotion analysis across the modality of vision and enriches modeling in an\naffect-aware setting."}
{"id": "2509.19611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19611", "abs": "https://arxiv.org/abs/2509.19611", "authors": ["Syeda Jannatus Saba", "Steven Skiena"], "title": "Evaluating Language Translation Models by Playing Telephone", "comment": "Accepted to EMNLP 2025 Main Conference as a long paper", "summary": "Our ability to efficiently and accurately evaluate the quality of machine\ntranslation systems has been outrun by the effectiveness of current language\nmodels--which limits the potential for further improving these models on more\nchallenging tasks like long-form and literary translation. We propose an\nunsupervised method to generate training data for translation evaluation over\ndifferent document lengths and application domains by repeated rounds of\ntranslation between source and target languages. We evaluate evaluation systems\ntrained on texts mechanically generated using both model rotation and language\ntranslation approaches, demonstrating improved performance over a popular\ntranslation evaluation system (xCOMET) on two different tasks: (i) scoring the\nquality of a given translation against a human reference and (ii) selecting\nwhich of two translations is generationally closer to an original source\ndocument."}
{"id": "2509.19640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19640", "abs": "https://arxiv.org/abs/2509.19640", "authors": ["Ryan Shea", "Zhou Yu"], "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification", "comment": "EMNLP Findings 2025", "summary": "Patents play a critical role in driving technological innovation by granting\ninventors exclusive rights to their inventions. However the process of drafting\na patent application is often expensive and time-consuming, making it a prime\ncandidate for automation. Despite recent advancements in language models,\nseveral challenges hinder the development of robust automated patent drafting\nsystems. First, the information within a patent application is highly\nconfidential, which often prevents the use of closed-source LLMs for automating\nthis task. Second, the process of drafting a patent application is difficult\nfor even the most advanced language models due to their long context, technical\nwriting style, and specialized domain knowledge. To address these challenges,\nwe introduce AutoSpec, a secure, agentic framework for Automatically drafting\npatent Specification. Our approach decomposes the drafting process into a\nsequence of manageable subtasks, each solvable by smaller, open-source language\nmodels enhanced with custom tools tailored for drafting patent specification.\nTo assess our system, we design a novel evaluation protocol in collaboration\nwith experienced patent attorneys. Our automatic and expert evaluations show\nthat AutoSpec outperforms existing baselines on a patent drafting task."}
{"id": "2509.19657", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.19657", "abs": "https://arxiv.org/abs/2509.19657", "authors": ["Yicheng Yang", "Zixian Li", "Jean Paul Bizimana", "Niaz Zafri", "Yongfeng Dong", "Tianyi Li"], "title": "Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections", "comment": null, "summary": "Pedestrian safety is a critical component of urban mobility and is strongly\ninfluenced by the interactions between pedestrian decision-making and driver\nyielding behavior at crosswalks. Modeling driver--pedestrian interactions at\nintersections requires accurately capturing the complexity of these behaviors.\nTraditional machine learning models often struggle to capture the nuanced and\ncontext-dependent reasoning required for these multifactorial interactions, due\nto their reliance on fixed feature representations and limited\ninterpretability. In contrast, large language models (LLMs) are suited for\nextracting patterns from heterogeneous traffic data, enabling accurate modeling\nof driver-pedestrian interactions. Therefore, this paper leverages multimodal\nLLMs through a novel prompt design that incorporates domain-specific knowledge,\nstructured reasoning, and few-shot prompting, enabling interpretable and\ncontext-aware inference of driver yielding behavior, as an example application\nof modeling pedestrian--driver interaction. We benchmarked state-of-the-art\nLLMs against traditional classifiers, finding that GPT-4o consistently achieves\nthe highest accuracy and recall, while Deepseek-V3 excels in precision. These\nfindings highlight the critical trade-offs between model performance and\ncomputational efficiency, offering practical guidance for deploying LLMs in\nreal-world pedestrian safety systems."}
{"id": "2509.19695", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19695", "abs": "https://arxiv.org/abs/2509.19695", "authors": ["Shuyu Zhang", "Yifan Wei", "Jialuo Yuan", "Xinru Wang", "Yanmin Zhu", "Bin Li"], "title": "DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems", "comment": null, "summary": "Task oriented dialog systems often rely on static exploration strategies that\ndo not adapt to dynamic dialog contexts, leading to inefficient exploration and\nsuboptimal performance. We propose DyBBT, a novel dialog policy learning\nframework that formalizes the exploration challenge through a structured\ncognitive state space capturing dialog progression, user uncertainty, and slot\ndependency. DyBBT proposes a bandit inspired meta-controller that dynamically\nswitches between a fast intuitive inference (System 1) and a slow deliberative\nreasoner (System 2) based on real-time cognitive states and visitation counts.\nExtensive experiments on single- and multi-domain benchmarks show that DyBBT\nachieves state-of-the-art performance in success rate, efficiency, and\ngeneralization, with human evaluations confirming its decisions are well\naligned with expert judgment. Code is available at\nhttps://github.com/carsonz/DyBBT."}
{"id": "2509.19727", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19727", "abs": "https://arxiv.org/abs/2509.19727", "authors": ["Seungjong Sun", "Seo Yeon Baek", "Jang Hyun Kim"], "title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging", "comment": "EMNLP 2025", "summary": "Driven by the demand for personalized AI systems, there is growing interest\nin aligning the behavior of large language models (LLMs) with human traits such\nas personality. Previous attempts to induce personality in LLMs have shown\npromising results, but they struggle to capture the continuous and\nmultidimensional nature of human traits. In this work, we propose a novel\nmethod for personality modulation in LLMs via model merging. Specifically, we\nconstruct personality vectors by subtracting the weights of a pre-trained model\nfrom those of the fine-tuned model on a given personality trait. By merging\npersonality vectors, we enable LLMs to exhibit desired personality traits\nwithout additional training. Extensive experiments show that personality\nvectors enable continuous control over trait intensity and support the\ncomposition of multiple traits. Furthermore, personality vectors transfer\nacross diverse downstream models, suggesting that they encode generalizable\nrepresentations of personality. Our code is available at here."}
{"id": "2509.19742", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19742", "abs": "https://arxiv.org/abs/2509.19742", "authors": ["Shuyu Zhang", "Yifan Wei", "Xinru Wang", "Yanmin Zhu", "Yangfan He", "Yixuan Weng", "Bin Li"], "title": "HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST", "comment": null, "summary": "Zero-shot Dialog State Tracking (zs-DST) is essential for enabling\nTask-Oriented Dialog Systems (TODs) to generalize to new domains without costly\ndata annotation. A central challenge lies in the semantic misalignment between\ndynamic dialog contexts and static prompts, leading to inflexible cross-layer\ncoordination, domain interference, and catastrophic forgetting. To tackle this,\nwe propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a\nframework that enhances zero-shot slot inference through robust prompt\nalignment. It features a hierarchical LoRA architecture for dynamic\nlayer-specific processing (combining lower-layer heuristic grouping and\nhigher-layer full interaction), integrates Spectral Joint Domain-Slot\nClustering to identify transferable associations (feeding an Adaptive Linear\nFusion Mechanism), and employs Semantic-Enhanced SVD Initialization\n(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain\ndatasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving\nSOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA."}
{"id": "2509.19745", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19745", "abs": "https://arxiv.org/abs/2509.19745", "authors": ["Pei Zhang", "Andong Chen", "Xi Chen", "Baosong Yang", "Derek F. Wong", "Fei Huang"], "title": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs", "comment": null, "summary": "Large language models (LLMs) have expanded from text to speech, giving rise\nto Speech Large Models (SLMs) that support recognition, translation, and\nsynthesis. A key challenge is aligning speech and text representations, which\nbecomes harder in multilingual settings. Existing methods often freeze LLM\nparameters and train encoders on multilingual data, but this forces\ncross-language convergence and limits performance. We introduce Progressive\nAlignment Representation Training (PART), a multi-stage and multi-task\nframework that separates within-language from cross-language alignment. During\ncross-language training, LLM parameters are dynamically activated, and\ntext-based tasks are later introduced to enhance multilingual understanding.\nExperiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART\nsurpasses conventional approaches, with analysis confirming its ability to\nbalance language-specific distinctions and cross-language generalization. These\nresults demonstrate PART's effectiveness and generality for multilingual speech\nmodality alignment."}
{"id": "2509.19768", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19768", "abs": "https://arxiv.org/abs/2509.19768", "authors": ["Sina J. Semnani", "Han Zhang", "Xinyan He", "Merve Tekg√ºrler", "Monica S. Lam"], "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition", "comment": "EMNLP 2025", "summary": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship."}
{"id": "2509.19770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19770", "abs": "https://arxiv.org/abs/2509.19770", "authors": ["Sen Yang", "Yu Bao", "Yu Lu", "Jiajun Chen", "Shujian Huang", "Shanbo Cheng"], "title": "EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation", "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) have demonstrated strong machine translation\ncapabilities for English-centric language pairs but underperform in direct\nnon-English (x2x) translation. This work addresses this limitation through a\nsynthetic data generation framework that leverages models' established\nEnglish-to-x (en2x) capabilities. By extending English parallel corpora into\nomnidirectional datasets and developing an English-referenced quality\nevaluation proxy, we enable effective collection of high-quality x2x training\ndata. Combined with preference-based optimization, our method achieves\nsignificant improvement across 72 x2x directions for widely used LLMs, while\ngeneralizing to enhance en2x performance. The results demonstrate that\nstrategic exploitation of English-centric strengths can bootstrap comprehensive\nmultilingual translation capabilities in LLMs. We release codes, datasets, and\nmodel checkpoints at https://github.com/NJUNLP/EAX"}
{"id": "2509.19775", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19775", "abs": "https://arxiv.org/abs/2509.19775", "authors": ["Wence Ji", "Jiancan Wu", "Aiying Li", "Shuyi Zhang", "Junkang Wu", "An Zhang", "Xiang Wang", "Xiangnan He"], "title": "bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), their robustness\nagainst adversarial manipulations, particularly jailbreak backdoor attacks, has\nbecome critically important. Existing approaches to embedding jailbreak\ntriggers--such as supervised fine-tuning (SFT), model editing, and\nreinforcement learning from human feedback (RLHF)--each suffer from limitations\nincluding poor generalization, compromised stealthiness, or reduced contextual\nusability of generated jailbreak responses. To overcome these issues, we\npropose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel\nRL-based framework tailored explicitly for jailbreak backdoor injection. By\nemploying pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the\nmodel to reliably produce harmful content with triggers and maintain safety\notherwise. Our approach leverages a rule-based reward mechanism complemented by\nlength and format incentives, eliminating dependence on high-quality supervised\ndatasets or potentially flawed reward models. Extensive experiments demonstrate\nthat bi-GRPO achieves superior effectiveness (>99\\% attack success rate),\npreserves stealthiness in non-trigger scenarios, and produces highly usable and\ncoherent jailbreak responses, significantly advancing the state-of-the-art in\njailbreak backdoor attacks."}
{"id": "2509.19833", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.19833", "abs": "https://arxiv.org/abs/2509.19833", "authors": ["Andrea Cadeddua", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "title": "Polarity Detection of Sustainable Detection Goals in News Text", "comment": null, "summary": "The United Nations' Sustainable Development Goals (SDGs) provide a globally\nrecognised framework for addressing critical societal, environmental, and\neconomic challenges. Recent developments in natural language processing (NLP)\nand large language models (LLMs) have facilitated the automatic classification\nof textual data according to their relevance to specific SDGs. Nevertheless, in\nmany applications, it is equally important to determine the directionality of\nthis relevance; that is, to assess whether the described impact is positive,\nneutral, or negative. To tackle this challenge, we propose the novel task of\nSDG polarity detection, which assesses whether a text segment indicates\nprogress toward a specific SDG or conveys an intention to achieve such\nprogress. To support research in this area, we introduce SDG-POD, a benchmark\ndataset designed specifically for this task, combining original and\nsynthetically generated data. We perform a comprehensive evaluation using six\nstate-of-the-art large LLMs, considering both zero-shot and fine-tuned\nconfigurations. Our results suggest that the task remains challenging for the\ncurrent generation of LLMs. Nevertheless, some fine-tuned models, particularly\nQWQ-32B, achieve good performance, especially on specific Sustainable\nDevelopment Goals such as SDG-9 (Industry, Innovation and Infrastructure),\nSDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).\nFurthermore, we demonstrate that augmenting the fine-tuning dataset with\nsynthetically generated examples yields improved model performance on this\ntask. This result highlights the effectiveness of data enrichment techniques in\naddressing the challenges of this resource-constrained domain. This work\nadvances the methodological toolkit for sustainability monitoring and provides\nactionable insights into the development of efficient, high-performing polarity\ndetection systems."}
{"id": "2509.19834", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19834", "abs": "https://arxiv.org/abs/2509.19834", "authors": ["Ji Yin", "Menglan He", "Yujie Zhang", "Linshuai Zhang", "Tingting Ma", "Ce Tian", "Jie Wu", "Lin Xu", "Tao Jiang"], "title": "TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios", "comment": "46 pages, 5 figures,3 tables", "summary": "Domain-specific LLMs in TCM face limitations in research settings due to\nconstrained adaptability, insufficient evaluation datasets, and limited\ncomputational resources. This study presents TianHui, a specialized TCM LLM\nbuilt through contextual data integration and domain knowledge fusion. We\nconstructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA\npairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage\n2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked\ntop-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)\nand achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,\nADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,\nepoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation\nand scalable application of TCM knowledge. All resources are open-sourced."}
{"id": "2509.19844", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19844", "abs": "https://arxiv.org/abs/2509.19844", "authors": ["Sujoy Sarkar", "Gourav Sarkar", "Manoj Balaji Jagadeeshan", "Jivnesh Sandhan", "Amrith Krishna", "Pawan Goyal"], "title": "MahƒÅnƒÅma: A Unique Testbed for Literary Entity Discovery and Linking", "comment": "Accepted to EMNLP 2025. This is the authors' version. The official\n  version will appear in the ACL Anthology", "summary": "High lexical variation, ambiguous references, and long-range dependencies\nmake entity resolution in literary texts particularly challenging. We present\nMah\\={a}n\\={a}ma, the first large-scale dataset for end-to-end Entity Discovery\nand Linking (EDL) in Sanskrit, a morphologically rich and under-resourced\nlanguage. Derived from the Mah\\={a}bh\\={a}rata, the world's longest epic, the\ndataset comprises over 109K named entity mentions mapped to 5.5K unique\nentities, and is aligned with an English knowledge base to support\ncross-lingual linking. The complex narrative structure of Mah\\={a}n\\={a}ma,\ncoupled with extensive name variation and ambiguity, poses significant\nchallenges to resolution systems. Our evaluation reveals that current\ncoreference and entity linking models struggle when evaluated on the global\ncontext of the test set. These results highlight the limitations of current\napproaches in resolving entities within such complex discourse. Mah\\=an\\=ama\nthus provides a unique benchmark for advancing entity resolution, especially in\nliterary domains."}
{"id": "2509.19858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19858", "abs": "https://arxiv.org/abs/2509.19858", "authors": ["Jinyang Wu", "Bin Zhu", "Xiandong Zou", "Qiquan Zhang", "Xu Fang", "Pan Zhou"], "title": "Benchmarking Gaslighting Attacks Against Speech Large Language Models", "comment": "5 pages, 2 figures, 3 tables", "summary": "As Speech Large Language Models (Speech LLMs) become increasingly integrated\ninto voice-based applications, ensuring their robustness against manipulative\nor adversarial input becomes critical. Although prior work has studied\nadversarial attacks in text-based LLMs and vision-language models, the unique\ncognitive and perceptual challenges of speech-based interaction remain\nunderexplored. In contrast, speech presents inherent ambiguity, continuity, and\nperceptual diversity, which make adversarial attacks more difficult to detect.\nIn this paper, we introduce gaslighting attacks, strategically crafted prompts\ndesigned to mislead, override, or distort model reasoning as a means to\nevaluate the vulnerability of Speech LLMs. Specifically, we construct five\nmanipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and\nProfessional Negation, designed to test model robustness across varied tasks.\nIt is worth noting that our framework captures both performance degradation and\nbehavioral responses, including unsolicited apologies and refusals, to diagnose\ndifferent dimensions of susceptibility. Moreover, acoustic perturbation\nexperiments are conducted to assess multi-modal robustness. To quantify model\nvulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on\nover 10,000 test samples from 5 diverse datasets reveals an average accuracy\ndrop of 24.3% under the five gaslighting attacks, indicating significant\nbehavioral vulnerability. These findings highlight the need for more resilient\nand trustworthy speech-based AI systems."}
{"id": "2509.19861", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19861", "abs": "https://arxiv.org/abs/2509.19861", "authors": ["Alba Maria Marmol-Romero", "Manuel Garcia-Vega", "Miguel Angel Garcia-Cumbreras", "Arturo Montejo-Raez"], "title": "SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection", "comment": "16 pages, 10 figures, 8 tables. CLEF (Working Notes). 2025", "summary": "This paper describes the participation of the SINAI-UJA team in the\neRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)\nTask 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:\nConversational Depression Detection via LLMs. Our approach for Task 2 combines\nan extensive preprocessing pipeline with the use of several transformer-based\nmodels, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual\nand sequential nature of multi-user conversations. For the Pilot Task, we\ndesigned a set of conversational strategies to interact with LLM-powered\npersonas, focusing on maximizing information gain within a limited number of\ndialogue turns. In Task 2, our system ranked 8th out of 12 participating teams\nbased on F1 score. However, a deeper analysis revealed that our models were\namong the fastest in issuing early predictions, which is a critical factor in\nreal-world deployment scenarios. This highlights the trade-off between early\ndetection and classification accuracy, suggesting potential avenues for\noptimizing both jointly in future work. In the Pilot Task, we achieved 1st\nplace out of 5 teams, obtaining the best overall performance across all\nevaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates\nthe effectiveness of structured conversational design when combined with\npowerful language models, reinforcing the feasibility of deploying LLMs in\nsensitive mental health assessment contexts."}
{"id": "2509.19866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19866", "abs": "https://arxiv.org/abs/2509.19866", "authors": ["Samuel Stucki", "Mark Cieliebak", "Jan Deriu"], "title": "SwissGPC v1.0 -- The Swiss German Podcasts Corpus", "comment": null, "summary": "We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous\nSwiss German speech, developed to support research in ASR, TTS, dialect\nidentification, and related fields. The dataset consists of links to talk shows\nand podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain\napproximately 5400 hours of raw audio. After segmentation and weak annotation,\nnearly 5000 hours of speech were retained, covering the seven major Swiss\nGerman dialect regions alongside Standard German. We describe the corpus\nconstruction methodology, including an automated annotation pipeline, and\nprovide statistics on dialect distribution, token counts, and segmentation\ncharacteristics. Unlike existing Swiss German speech corpora, which primarily\nfeature controlled speech, this corpus captures natural, spontaneous\nconversations, making it a valuable resource for real-world speech\napplications."}
{"id": "2509.19880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19880", "abs": "https://arxiv.org/abs/2509.19880", "authors": ["Wei-Hsiang Lin", "Sheng-Lun Wei", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation", "comment": "Accepted as a long findings paper at EMNLP 2025", "summary": "LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet\nresearch findings on the relationship between models' generation and judgment\nabilities remain inconsistent. We investigate this relationship through\nsystematic dataset- and instance-level analyses across 11 models and 21 diverse\ntasks. Despite both capabilities relying on the same underlying knowledge, our\nanalyses reveal they are only weakly correlated, primarily due to LLMs'\nsensitivity to the responses being judged. To address this, we propose a\nself-reference-guided evaluation strategy that leverages a model's own answers\nas references. This approach significantly strengthens the correlation between\ngeneration and judgment abilities, offering a practical path to align these\nskills and providing a reliable proxy for model selection in evaluation tasks."}
{"id": "2509.19893", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19893", "abs": "https://arxiv.org/abs/2509.19893", "authors": ["Minjae Oh", "Yunho Choi", "Dongmin Choi", "Yohan Jo"], "title": "Future Policy Aware Preference Learning for Mathematical Reasoning", "comment": "9 pages", "summary": "Preference learning methods such as Direct Preference Optimization (DPO) have\nbecome standard for Large Language Model (LLM) post-training, yet they are\noften ineffective for mathematical reasoning. A key challenge is the large\ntoken overlap between preferred and dispreferred trajectories; lowering the\nprobability of dispreferred trajectories also reduces the probability of shared\nuseful tokens, leading to over-penalization and overall performance collapse.\nAs a mitigation, existing algorithms include the probability of a trajectory\nunder the current policy as a regularization term, which decreases the effect\nof the gradient when the probability is low. However, by the time this effect\ntakes hold, useful tokens may have already been over-penalized as the model has\nbegun to degrade. To address this, we propose Future Policy Aware (FPA)\npreference learning, which replaces the current policy with a future policy in\nthe regularization term. This future policy is estimated via lightweight,\nlogit-space extrapolation from a reference model toward the current model. FPA\nenables safer training by preemptively regularizing potentially problematic\ngradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH\nand GSM8K benchmarks. FPA yields consistent performance gains, with the largest\nimprovements observed with SimPER, achieving gains of up to 5.75%. We\ndemonstrate that FPA provides proactive regularization while preserving the\nprobability of shared, useful mathematical tokens, and enables longer,\ndegradation-free training with negligible computational overhead. We will\nrelease our code publicly upon publication."}
{"id": "2509.19902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19902", "abs": "https://arxiv.org/abs/2509.19902", "authors": ["Binbin Zhang", "Chengdong Liang", "Shuai Wang", "Xuelong Geng", "Zhao Guo", "Haoyu Li", "Hao Yin", "Xipeng Yang", "Pengshen Zhang", "Changwei Ma", "Lei Xie"], "title": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction", "comment": null, "summary": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/"}
{"id": "2509.19941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19941", "abs": "https://arxiv.org/abs/2509.19941", "authors": ["Soham Bhattacharjee", "Mukund K Roy", "Yathish Poojary", "Bhargav Dave", "Mihir Raj", "Vandan Mujadia", "Baban Gain", "Pruthwik Mishra", "Arafat Ahsan", "Parameswari Krishnamurthy", "Ashwath Rao", "Gurpreet Singh Josan", "Preeti Dubey", "Aadil Amin Kak", "Anna Rao Kulkarni", "Narendra VG", "Sunita Arora", "Rakesh Balbantray", "Prasenjit Majumdar", "Karunesh K Arora", "Asif Ekbal", "Dipti Mishra Sharma"], "title": "CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems", "comment": null, "summary": "India's linguistic landscape is one of the most diverse in the world,\ncomprising over 120 major languages and approximately 1,600 additional\nlanguages, with 22 officially recognized as scheduled languages in the Indian\nConstitution. Despite recent progress in multilingual neural machine\ntranslation (NMT), high-quality parallel corpora for Indian languages remain\nscarce, especially across varied domains. In this paper, we introduce a\nlarge-scale, high-quality annotated parallel corpus covering 11 of these\nlanguages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,\nKannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence\npairs. The dataset is carefully curated and systematically categorized into\nthree key domains: Government, Health, and General, to enable domain-aware\nmachine translation research and facilitate effective domain adaptation. To\ndemonstrate the utility of CorIL and establish strong benchmarks for future\nresearch, we fine-tune and evaluate several state-of-the-art NMT models,\nincluding IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important\nperformance trends and highlights the corpus's value in probing model\ncapabilities. For instance, the results show distinct performance patterns\nbased on language script, with massively multilingual models showing an\nadvantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on\nIndic scripts. This paper provides a detailed domain-wise performance analysis,\noffering insights into domain sensitivity and cross-script transfer learning.\nBy publicly releasing CorIL, we aim to significantly improve the availability\nof high-quality training data for Indian languages and provide a valuable\nresource for the machine translation research community."}
{"id": "2509.20004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20004", "abs": "https://arxiv.org/abs/2509.20004", "authors": ["Jan Broersen"], "title": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots", "comment": null, "summary": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse."}
{"id": "2509.20007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20007", "abs": "https://arxiv.org/abs/2509.20007", "authors": ["Kota Dohi", "Tomoya Nishida", "Harsh Purohit", "Takashi Endo", "Yohei Kawaguchi"], "title": "DiffNator: Generating Structured Explanations of Time-Series Differences", "comment": null, "summary": "In many IoT applications, the central interest lies not in individual sensor\nsignals but in their differences, yet interpreting such differences requires\nexpert knowledge. We propose DiffNator, a framework for structured explanations\nof differences between two time series. We first design a JSON schema that\ncaptures the essential properties of such differences. Using the Time-series\nObservations of Real-world IoT (TORI) dataset, we generate paired sequences and\ntrain a model that combine a time-series encoder with a frozen LLM to output\nJSON-formatted explanations. Experimental results show that DiffNator generates\naccurate difference explanations and substantially outperforms both a visual\nquestion answering (VQA) baseline and a retrieval method using a pre-trained\ntime-series encoder."}
{"id": "2509.20045", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20045", "abs": "https://arxiv.org/abs/2509.20045", "authors": ["Vani Kanjirangat", "Tanja Samard≈æiƒá", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks", "comment": "Accepted in EMNLP-2025 Main conference", "summary": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level."}
{"id": "2509.20057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20057", "abs": "https://arxiv.org/abs/2509.20057", "authors": ["KT", ":", "Soonmin Bae", "Wanjin Park", "Jeongyeop Kim", "Yunjin Park", "Jungwon Yoon", "Junhyung Moon", "Myunggyo Oh", "Wonhyuk Lee", "Junseo Jang", "Dongyoung Jung", "Minwook Ju", "Eunmi Kim", "Sujin Kim", "Youngchol Kim", "Somin Lee", "Wonyoung Lee", "Minsung Noh", "Hyoungjun Park", "Eunyoung Shin"], "title": "Responsible AI Technical Report", "comment": "23 pages, 8 figures", "summary": "KT developed a Responsible AI (RAI) assessment methodology and risk\nmitigation technologies to ensure the safety and reliability of AI services. By\nanalyzing the Basic Act on AI implementation and global AI governance trends,\nwe established a unique approach for regulatory compliance and systematically\nidentify and manage all potential risk factors from AI development to\noperation. We present a reliable assessment methodology that systematically\nverifies model safety and robustness based on KT's AI risk taxonomy tailored to\nthe domestic environment. We also provide practical tools for managing and\nmitigating identified AI risks. With the release of this report, we also\nrelease proprietary Guardrail : SafetyGuard that blocks harmful responses from\nAI models in real-time, supporting the enhancement of safety in the domestic AI\ndevelopment ecosystem. We also believe these research outcomes provide valuable\ninsights for organizations seeking to develop Responsible AI."}
{"id": "2509.20065", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20065", "abs": "https://arxiv.org/abs/2509.20065", "authors": ["Maggie Mi", "Aline Villavicencio", "Nafise Sadat Moosavi"], "title": "From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors", "comment": "EMNLP 2025", "summary": "Language models often struggle with idiomatic, figurative, or\ncontext-sensitive inputs, not because they produce flawed outputs, but because\nthey misinterpret the input from the outset. We propose an input-only method\nfor anticipating such failures using token-level likelihood features inspired\nby surprisal and the Uniform Information Density hypothesis. These features\ncapture localized uncertainty in input comprehension and outperform standard\nbaselines across five linguistically challenging datasets. We show that\nspan-localized features improve error detection for larger models, while\nsmaller models benefit from global patterns. Our method requires no access to\noutputs or hidden activations, offering a lightweight and generalizable\napproach to pre-generation error prediction."}
{"id": "2509.20072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20072", "abs": "https://arxiv.org/abs/2509.20072", "authors": ["Tianqiao Liu", "Xueyi Li", "Hao Wang", "Haoxuan Li", "Zhichao Chen", "Weiqi Luo", "Zitao Liu"], "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training", "comment": null, "summary": "Recent advances in large language models have attracted significant interest\nin extending their capabilities to multimodal scenarios, particularly for\nspeech-in speech-out conversational systems. However, existing multimodal\nmodels handling interleaved audio and text, such as MOSHI require complex multi\nstage training pipelines, incurring substantial computational costs. Moreover,\nthese models uniformly apply autoregressive generation to both text and audio\ntokens, overlooking a fundamental asymmetry in their dependency structures:\nwhile text tokens exhibit strong target target dependencies requiring causal\nordering, audio tokens are predominantly driven by source target dependencies,\nwhere audio outputs primarily condition on source text rather than preceding\naudio tokens. In this work, we propose TtT, a unified audio-text modeling\nframework that integrates AR text generation with non-autoregressive audio\ndiffusion within a single Transformer architecture initialized from a\npretrained LLM."}
{"id": "2509.20074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20074", "abs": "https://arxiv.org/abs/2509.20074", "authors": ["Ganesh Katrapati", "Manish Shrivastava"], "title": "Can Constructions \"SCAN\" Compositionality ?", "comment": null, "summary": "Sequence to Sequence models struggle at compositionality and systematic\ngeneralisation even while they excel at many other tasks. We attribute this\nlimitation to their failure to internalise constructions conventionalised form\nmeaning pairings that license productive recombination. Building on these\ninsights, we introduce an unsupervised procedure for mining\npseudo-constructions: variable-slot templates automatically extracted from\ntraining data. When applied to the SCAN dataset, our method yields large gains\nout-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on\nAROUND RIGHT without any architectural changes or additional supervision. The\nmodel also attains competitive performance with? 40% of the original training\ndata, demonstrating strong data efAciency. Our findings highlight the promise\nof construction-aware preprocessing as an alternative to heavy architectural or\ntraining-regime interventions."}
{"id": "2509.20086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20086", "abs": "https://arxiv.org/abs/2509.20086", "authors": ["Johannes Wirth"], "title": "OLaPh: Optimal Language Phonemizer", "comment": "5 pages, 1 figure, 3 tables", "summary": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research."}
{"id": "2509.20088", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20088", "abs": "https://arxiv.org/abs/2509.20088", "authors": ["Oscar Lithgow-Serrano", "Vani Kanjirangat", "Alessandro Antonucci"], "title": "Causal Understanding by LLMs: The Role of Uncertainty", "comment": "Accepted in second UncertaiNLP workshop at EMNLP 2025", "summary": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining."}
{"id": "2509.20097", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20097", "abs": "https://arxiv.org/abs/2509.20097", "authors": ["Sujeong Lee", "Hayoung Lee", "Seongsoo Heo", "Wonik Choi"], "title": "Integrated Framework for LLM Evaluation with Answer Generation", "comment": "16pages", "summary": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies."}
{"id": "2509.20129", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20129", "abs": "https://arxiv.org/abs/2509.20129", "authors": ["York Hay Ng", "Phuong Hanh Hoang", "En-Shiun Annie Lee"], "title": "Less is More: The Effectiveness of Compact Typological Language Representations", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Linguistic feature datasets such as URIEL+ are valuable for modelling\ncross-lingual relationships, but their high dimensionality and sparsity,\nespecially for low-resource languages, limit the effectiveness of distance\nmetrics. We propose a pipeline to optimize the URIEL+ typological feature space\nby combining feature selection and imputation, producing compact yet\ninterpretable typological representations. We evaluate these feature subsets on\nlinguistic distance alignment and downstream tasks, demonstrating that\nreduced-size representations of language typology can yield more informative\ndistance metrics and improve performance in multilingual NLP applications."}
{"id": "2509.20162", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20162", "abs": "https://arxiv.org/abs/2509.20162", "authors": ["Chaojun Nie", "Jun Zhou", "Guanxiang Wang", "Shisong Wud", "Zichen Wang"], "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation", "comment": null, "summary": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG."}
{"id": "2509.20168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20168", "abs": "https://arxiv.org/abs/2509.20168", "authors": ["Ghazal Kalhor", "Behnam Bahrak"], "title": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian", "comment": "Accepted and forthcoming at the Widening Natural Language Processing\n  Workshop (WiNLP 2025) at EMNLP 2025", "summary": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages."}
{"id": "2509.20186", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20186", "abs": "https://arxiv.org/abs/2509.20186", "authors": ["Liang Wang", "Nan Yang", "Shaohan Huang", "Li Dong", "Furu Wei"], "title": "Thinking Augmented Pre-training", "comment": "19 pages", "summary": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks."}
{"id": "2509.20208", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.20208", "abs": "https://arxiv.org/abs/2509.20208", "authors": ["Parker Glenn", "Alfy Samuel", "Daben Liu"], "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs", "comment": null, "summary": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql"}
{"id": "2509.20209", "categories": ["cs.CL", "cs.AI", "68T50, 68T35", "I.2.7; H.3.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.20209", "abs": "https://arxiv.org/abs/2509.20209", "authors": ["Hailay Kidu Teklehaymanot", "Gebrearegawi Gidey", "Wolfgang Nejdl"], "title": "Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks", "comment": "This submission is 8 pages long, includes 4 tables, and contains all\n  required conference details", "summary": "Despite advances in Neural Machine Translation (NMT), low-resource languages\nlike Tigrinya remain underserved due to persistent challenges, including\nlimited corpora, inadequate tokenization strategies, and the lack of\nstandardized evaluation benchmarks. This paper investigates transfer learning\ntechniques using multilingual pretrained models to enhance translation quality\nfor morphologically rich, low-resource languages. We propose a refined approach\nthat integrates language-specific tokenization, informed embedding\ninitialization, and domain-adaptive fine-tuning. To enable rigorous assessment,\nwe construct a high-quality, human-aligned English-Tigrinya evaluation dataset\ncovering diverse domains. Experimental results demonstrate that transfer\nlearning with a custom tokenizer substantially outperforms zero-shot baselines,\nwith gains validated by BLEU, chrF, and qualitative human evaluation.\nBonferroni correction is applied to ensure statistical significance across\nconfigurations. Error analysis reveals key limitations and informs targeted\nrefinements. This study underscores the importance of linguistically aware\nmodeling and reproducible benchmarks in bridging the performance gap for\nunderrepresented languages. Resources are available at\nhttps://github.com/hailaykidu/MachineT_TigEng\n  and https://huggingface.co/Hailay/MachineT_TigEng"}
{"id": "2509.20237", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20237", "abs": "https://arxiv.org/abs/2509.20237", "authors": ["Yu Wang", "Leyi Lao", "Langchu Huang", "Gabriel Skantze", "Yang Xu", "Hendrik Buschmeier"], "title": "Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models", "comment": null, "summary": "Backchannels and fillers are important linguistic expressions in dialogue,\nbut are under-represented in modern transformer-based language models (LMs).\nOur work studies the representation of them in language models using three\nfine-tuning strategies. The models are trained on three dialogue corpora in\nEnglish and Japanese, where backchannels and fillers are preserved and\nannotated, to investigate how fine-tuning can help LMs learn their\nrepresentations. We first apply clustering analysis to the learnt\nrepresentation of backchannels and fillers, and have found increased silhouette\nscores in representations from fine-tuned models, which suggests that\nfine-tuning enables LMs to distinguish the nuanced semantic variation in\ndifferent backchannel and filler use. We also use natural language generation\n(NLG) metrics to confirm that the utterances generated by fine-tuned language\nmodels resemble human-produced utterances more closely. Our findings suggest\nthe potentials of transforming general LMs into conversational LMs that are\nmore capable of producing human-like languages adequately."}
{"id": "2509.20278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20278", "abs": "https://arxiv.org/abs/2509.20278", "authors": ["Zipeng Ling", "Yuehao Tang", "Chen Huang", "Shuliang Liu", "Gaoyang Jiang", "Shenghong Fu", "Junqi Yang", "Yao Wan", "Jiawan Zhang", "Kejia Huang", "Xuming Hu"], "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage", "comment": null, "summary": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully."}
{"id": "2509.20287", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20287", "abs": "https://arxiv.org/abs/2509.20287", "authors": ["Behzad Shayegh", "Jan-Thorsten Peter", "David Vilar", "Tobias Domhan", "Juraj Juraska", "Markus Freitag", "Lili Mou"], "title": "Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation", "comment": "Accepted by Tenth Conference on Machine Translation (WMT25)", "summary": "We investigate the tradeoff between adequacy and fluency in machine\ntranslation. We show the severity of this tradeoff at the evaluation level and\nanalyze where popular metrics fall within it. Essentially, current metrics\ngenerally lean toward adequacy, meaning that their scores correlate more\nstrongly with the adequacy of translations than with fluency. More importantly,\nwe find that this tradeoff also persists at the meta-evaluation level, and that\nthe standard WMT meta-evaluation favors adequacy-oriented metrics over\nfluency-oriented ones. We show that this bias is partially attributed to the\ncomposition of the systems included in the meta-evaluation datasets. To control\nthis bias, we propose a method that synthesizes translation systems in\nmeta-evaluation. Our findings highlight the importance of understanding this\ntradeoff in meta-evaluation and its impact on metric rankings."}
{"id": "2509.20315", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20315", "abs": "https://arxiv.org/abs/2509.20315", "authors": ["T. O. Abiola", "K. D. Abiodun", "O. E. Olumide", "O. O. Adebanji", "O. Hiram Calvo", "Grigori Sidorov"], "title": "Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning", "comment": null, "summary": "Hope speech language that fosters encouragement and optimism plays a vital\nrole in promoting positive discourse online. However, its detection remains\nchallenging, especially in multilingual and low-resource settings. This paper\npresents a multilingual framework for hope speech detection using an active\nlearning approach and transformer-based models, including mBERT and\nXLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,\nGerman, and Urdu, including benchmark test sets from recent shared tasks. Our\nresults show that transformer models significantly outperform traditional\nbaselines, with XLM-RoBERTa achieving the highest overall accuracy.\nFurthermore, our active learning strategy maintained strong performance even\nwith small annotated datasets. This study highlights the effectiveness of\ncombining multilingual transformers with data-efficient training strategies for\nhope speech detection."}
{"id": "2509.20317", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20317", "abs": "https://arxiv.org/abs/2509.20317", "authors": ["Xilin Wei", "Xiaoran Liu", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Jiaqi Wang", "Xipeng Qiu", "Dahua Lin"], "title": "SIM-CoT: Supervised Implicit Chain-of-Thought", "comment": null, "summary": "Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning in Large Language Models (LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a core latent instability issue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing their semantic diversity, a\nfailure caused by insufficient step-level supervision in existing implicit CoT\napproaches. To address this issue, we propose SIM-CoT, a plug-and-play training\nmodule that introduces step-level supervision to stabilize and enrich the\nlatent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder\nduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposed auxiliary decoder is removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, the auxiliary decoder affords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and diagnosis.\nSIM-CoT significantly enhances both the in-domain accuracy and out-of-domain\nstability of various implicit CoT methods, boosting baselines like Coconut by\n+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong\nscalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%\nwith 2.3\\times greater token efficiency, while substantially closing the\nperformance gap on larger models like LLaMA-3.1 8B."}
{"id": "2509.20319", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20319", "abs": "https://arxiv.org/abs/2509.20319", "authors": ["Maria Teleki", "Sai Janjur", "Haoran Liu", "Oliver Grabner", "Ketan Verma", "Thomas Docog", "Xiangjue Dong", "Lingfeng Shi", "Cong Wang", "Stephanie Birkelbach", "Jason Kim", "Yin Zhang", "James Caverlee"], "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal", "comment": null, "summary": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies."}
{"id": "2509.20321", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20321", "abs": "https://arxiv.org/abs/2509.20321", "authors": ["Maria Teleki", "Sai Janjur", "Haoran Liu", "Oliver Grabner", "Ketan Verma", "Thomas Docog", "Xiangjue Dong", "Lingfeng Shi", "Cong Wang", "Stephanie Birkelbach", "Jason Kim", "Yin Zhang", "James Caverlee"], "title": "DRES: Benchmarking LLMs for Disfluency Removal", "comment": null, "summary": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems."}
{"id": "2509.20341", "categories": ["cs.CL", "cs.AI", "68T50, 68T35, 68N01", "I.2.7; I.2.6; H.3.1"], "pdf": "https://arxiv.org/pdf/2509.20341", "abs": "https://arxiv.org/abs/2509.20341", "authors": ["Gebrearegawi Gebremariam", "Hailay Teklehaymanot", "Gebregewergs Mezgebe"], "title": "Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations", "comment": "13 pages,2 images,7 tables", "summary": "Ge'ez is an ancient Semitic language renowned for its unique alphabet. It\nserves as the script for numerous languages, including Tigrinya and Amharic,\nand played a pivotal role in Ethiopia's cultural and religious development\nduring the Aksumite kingdom era. Ge'ez remains significant as a liturgical\nlanguage in Ethiopia and Eritrea, with much of the national identity\ndocumentation recorded in Ge'ez. These written materials are invaluable primary\nsources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,\nand civilization. Ge'ez has a complex morphological structure with rich\ninflectional and derivational morphology, and no usable NLP has been developed\nand published until now due to the scarcity of annotated linguistic data,\ncorpora, labeled datasets, and lexicons. Therefore, we propose a rule-based\nGe'ez morphological synthesizer to generate surface words from root words\naccording to the morphological structures of the language. We used 1,102 sample\nverbs, representing all verb morphological structures, to test and evaluate the\nsystem. The system achieves a performance of 97.4%, outperforming the baseline\nmodel and suggesting that future work should build a comprehensive system\nconsidering morphological variations of the language.\n  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based"}
{"id": "2509.20354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20354", "abs": "https://arxiv.org/abs/2509.20354", "authors": ["Henrique Schechter Vera", "Sahil Dua", "Biao Zhang", "Daniel Salz", "Ryan Mullins", "Sindhu Raghuram Panyam", "Sara Smoot", "Iftekhar Naim", "Joe Zou", "Feiyang Chen", "Daniel Cer", "Alice Lisak", "Min Choi", "Lucas Gonzalez", "Omar Sanseviero", "Glenn Cameron", "Ian Ballantyne", "Kat Black", "Kaifeng Chen", "Weiyi Wang", "Zhe Li", "Gus Martins", "Jinhyuk Lee", "Mark Sherwood", "Juyeong Ji", "Renjie Wu", "Jingxiao Zheng", "Jyotinder Singh", "Abheesht Sharma", "Divya Sreepat", "Aashi Jain", "Adham Elarabawy", "AJ Co", "Andreas Doumanoglou", "Babak Samari", "Ben Hora", "Brian Potetz", "Dahun Kim", "Enrique Alfonseca", "Fedor Moiseev", "Feng Han", "Frank Palma Gomez", "Gustavo Hern√°ndez √Åbrego", "Hesen Zhang", "Hui Hui", "Jay Han", "Karan Gill", "Ke Chen", "Koert Chen", "Madhuri Shanbhogue", "Michael Boratko", "Paul Suganthan", "Sai Meher Karthik Duddu", "Sandeep Mariserla", "Setareh Ariafar", "Shanfeng Zhang", "Shijie Zhang", "Simon Baumgartner", "Sonam Goenka", "Steve Qiu", "Tanmaya Dabral", "Trevor Walker", "Vikram Rao", "Waleed Khawaja", "Wenlei Zhou", "Xiaoqi Ren", "Ye Xia", "Yichang Chen", "Yi-Ting Chen", "Zhe Dong", "Zhongli Ding", "Francesco Visin", "Ga√´l Liu", "Jiageng Zhang", "Kathleen Kenealy", "Michelle Casbon", "Ravin Kumar", "Thomas Mesnard", "Zach Gleicher", "Cormac Brick", "Olivier Lacombe", "Adam Roberts", "Yunhsuan Sung", "Raphael Hoffmann", "Tris Warkentin", "Armand Joulin", "Tom Duerig", "Mojtaba Seyedhosseini"], "title": "EmbeddingGemma: Powerful and Lightweight Text Representations", "comment": "18 pages. Models are available in HuggingFace (at\n  https://huggingface.co/collections/google/embeddinggemma-68b9ae3a72a82f0562a80dc4),\n  Kaggle (at https://www.kaggle.com/models/google/embeddinggemma/), and Vertex\n  AI (at\n  https://pantheon.corp.google.com/vertex-ai/publishers/google/model-garden/embeddinggemma)", "summary": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research."}
{"id": "2509.20357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20357", "abs": "https://arxiv.org/abs/2509.20357", "authors": ["Adithya Bhaskar", "Xi Ye", "Danqi Chen"], "title": "Language Models that Think, Chat Better", "comment": "Preprint; we release our code and models publicly at\n  https://github.com/princeton-pli/RLMT", "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model\nreasoning by using rule-based rewards in verifiable domains such as mathematics\nand code. However, RLVR leads to limited generalization for open-ended tasks --\nsuch as writing outline essays or making meal plans -- where humans reason\nroutinely. This paper shows that the RLVR paradigm is effective beyond\nverifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking\n(**RLMT**) for general-purpose chat capabilities. Using diverse real-world\nprompts, RLMT requires LMs to generate long CoT reasoning before response, and\noptimizes them with online RL against a preference-based reward model used in\nRLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and\ninstruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT\nconsistently outperforms standard RLHF pipelines. This includes substantial\ngains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and\nArenaHardV2), along with 1-3 point improvements on other tasks like creative\nwriting and general knowledge. Our best 8B model surpasses GPT-4o in chat and\ncreative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be\napplied directly to base models without an SFT stage, akin to R1-Zero training.\nRemarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT\nrecipe outperforms Llama-3.1-8B-Instruct post-trained with a complex\nmulti-staged pipeline with 25M+ examples. We close with qualitative and\nquantitative analyses of how trained models plan their responses. Our results\nrethink the post-training pipeline and call upon future work to understand and\nemploy thinking more broadly."}
